{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZwBMt6VJGeW-",
        "c1p1JzJWGhvH",
        "Sgh3esHf5Mmn",
        "7FYVCtRh6tvj",
        "ZKHPfHa962C-",
        "_uE1UbuG61cB",
        "eyiLYqgo7Kcv",
        "71-KPDCE7Kc0",
        "y7WAezGP7LX-",
        "-nGbUYoTs29F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical AI and MLOps : Assignment 1"
      ],
      "metadata": {
        "id": "WA4bucbhF6wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries and fetch data"
      ],
      "metadata": {
        "id": "gApLH-475U9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT EDIT\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-4LzDDmpH-v9mIgwDqa6cSF5XrVm-SXg' -O linear_regression.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-AtCVwrcpgFCvAD3gwkUz812DIxqK338' -O logistic_regression.pkl\n",
        "\n",
        "X1_train, X1_test, Y1_train, Y1_test, X2_train, X2_test, Y2_train, Y2_test = [[] for _ in range(8)]\n",
        "\n",
        "with open(\"/content/linear_regression.pkl\", 'rb') as f:\n",
        "    X1_train, X1_test, Y1_train, Y1_test = pickle.load(f)\n",
        "\n",
        "with open(\"/content/logistic_regression.pkl\", 'rb') as f:\n",
        "    X2_train, X2_test, Y2_train, Y2_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "15YVgp7KNjD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdba02d9-0dbe-48e2-9bb0-bf93f22d37b9"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-03 13:22:27--  https://docs.google.com/uc?export=download&id=1-4LzDDmpH-v9mIgwDqa6cSF5XrVm-SXg\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.138, 142.251.2.100, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-0s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fiv2sn1i1618e2ktj2j2nv1bm6asv9di/1693747275000/06549736550030827657/*/1-4LzDDmpH-v9mIgwDqa6cSF5XrVm-SXg?e=download&uuid=609ae1d7-fbd1-447b-81fd-b17693627a2f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-09-03 13:22:27--  https://doc-10-0s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fiv2sn1i1618e2ktj2j2nv1bm6asv9di/1693747275000/06549736550030827657/*/1-4LzDDmpH-v9mIgwDqa6cSF5XrVm-SXg?e=download&uuid=609ae1d7-fbd1-447b-81fd-b17693627a2f\n",
            "Resolving doc-10-0s-docs.googleusercontent.com (doc-10-0s-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-10-0s-docs.googleusercontent.com (doc-10-0s-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1847 (1.8K) [application/octet-stream]\n",
            "Saving to: ‘linear_regression.pkl’\n",
            "\n",
            "linear_regression.p 100%[===================>]   1.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-03 13:22:27 (113 MB/s) - ‘linear_regression.pkl’ saved [1847/1847]\n",
            "\n",
            "--2023-09-03 13:22:27--  https://docs.google.com/uc?export=download&id=1-AtCVwrcpgFCvAD3gwkUz812DIxqK338\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.138, 142.251.2.100, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0o-0s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/slrs7f228nnb5jb53upiaq9a7ug9dr7g/1693747275000/06549736550030827657/*/1-AtCVwrcpgFCvAD3gwkUz812DIxqK338?e=download&uuid=fa1d6dfe-1f8d-48c1-8b73-6d77428a8ca4 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-09-03 13:22:28--  https://doc-0o-0s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/slrs7f228nnb5jb53upiaq9a7ug9dr7g/1693747275000/06549736550030827657/*/1-AtCVwrcpgFCvAD3gwkUz812DIxqK338?e=download&uuid=fa1d6dfe-1f8d-48c1-8b73-6d77428a8ca4\n",
            "Resolving doc-0o-0s-docs.googleusercontent.com (doc-0o-0s-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-0o-0s-docs.googleusercontent.com (doc-0o-0s-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17088 (17K) [application/octet-stream]\n",
            "Saving to: ‘logistic_regression.pkl’\n",
            "\n",
            "logistic_regression 100%[===================>]  16.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-03 13:22:28 (113 MB/s) - ‘logistic_regression.pkl’ saved [17088/17088]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train = np.resize(X1_train, (len(X1_train), 1))\n",
        "X1_test =  np.resize(X1_test,  (len(X1_test), 1))\n",
        "Y1_train = np.resize(Y1_train, (len(Y1_train), 1))\n",
        "Y1_test =  np.resize(Y1_test,  (len(Y1_test), 1))\n",
        "print(X1_train.shape)\n",
        "print(X1_test.shape)\n",
        "print(Y1_train.shape)\n",
        "print(Y1_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ2uhp46S71m",
        "outputId": "2544b448-c178-4deb-d16d-3a7470408f0c"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 1)\n",
            "(30, 1)\n",
            "(70, 1)\n",
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y2_train = np.resize(Y2_train, (len(Y2_train), 1))\n",
        "Y2_test =  np.resize(Y2_test,  (len(Y2_test), 1))\n",
        "print(X2_train.shape)\n",
        "print(X2_test.shape)\n",
        "print(Y2_train.shape)\n",
        "print(Y2_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePFgBZx9S-rV",
        "outputId": "90ba4ecb-da85-4882-8722-c5b56eb1f2ef"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 20)\n",
            "(30, 20)\n",
            "(70, 1)\n",
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((X1_train**-1).shape)\n",
        "print((X1_train).shape)\n",
        "print(X1_test[0:5])\n",
        "print(Y1_train.shape)\n",
        "print(Y1_test[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz0K3Rh-TE7i",
        "outputId": "45fca100-0626-4877-9794-366928c8a1e7"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 1)\n",
            "(70, 1)\n",
            "[[ 6.26850875]\n",
            " [-5.75615029]\n",
            " [-5.98513959]\n",
            " [-9.76278328]\n",
            " [-6.04948187]]\n",
            "(70, 1)\n",
            "[[ 291.87875938]\n",
            " [-163.34294373]\n",
            " [-184.56229109]\n",
            " [-844.96063406]\n",
            " [-190.84148613]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X2_train[0])\n",
        "print(X2_test[0])\n",
        "print(Y2_train[0:5])\n",
        "print(Y2_test[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzudv35bTQ1i",
        "outputId": "21ba0b77-6d77-494f-b25b-c5a40e10c609"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.67857138  2.7670727   2.03131422  0.3322455  -1.2361682  -0.48420497\n",
            " -1.6533102   0.17205938 -1.14020401 -1.02420001 -1.41826043  0.26769137\n",
            "  0.39235085  0.49597238 -2.04503375  0.75743566  2.50947819  0.53326592\n",
            "  0.92774064 -1.3631717 ]\n",
            "[ 1.59980278  0.34209474  1.39349879 -1.44936033 -0.94399061  0.00816769\n",
            " -0.87028399 -1.49635521  0.66575811  1.06385996 -1.73467213  0.05441906\n",
            " -0.05293944  0.82191122  0.04120958 -1.71024922  0.49349984  0.124863\n",
            " -0.22401342  0.47682353]\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Linear Regression (2 Marks)"
      ],
      "metadata": {
        "id": "qMLBBS5ZGWUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a linear regression model for the above set of data. Use MSE(Mean Squared Error) as the loss function.\n",
        "\n",
        "Print out the train accuracy and test accuracy.\n",
        "\n",
        "**Data**: X1_train, X1_test, Y1_train, Y1_test"
      ],
      "metadata": {
        "id": "QUoMW8UzcLcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Solve the question using normal equation\n",
        "# Normal equation is Y1_train = regression coefficients or theta * X1_train\n",
        "# theta = Y1_train * inv(X1_train)\n",
        "theta = np.dot(Y1_train.T,(X1_train)**-1)\n",
        "print(\"theta:\", theta)\n",
        "\n",
        "## Define the hypothesis function for linear regression\n",
        "#Answer: Hypothesis y_hat =  beta_1*X1_train\n",
        "#Cost_function = (np.sum(Y1_train - (bias + weight_1*X1_train) ))/len(Y1_train)\n",
        "#print(Y1_train[0:5])\n",
        "\n",
        "## Use linear regression to train the model\n",
        "\n",
        "Regression_coefficient = (np.dot(X1_train.T,X1_train)**-1)*np.dot(X1_train.T,Y1_train)\n",
        "print(\"Regression_coefficient: \", Regression_coefficient)\n",
        "Y1_Lin_NoBias = Regression_coefficient*X1_train\n",
        "print(Y1_Lin_NoBias.shape)\n",
        "## Compute the training and testing loss using MSE error\n",
        "Post_training_error = np.sum((Y1_train - Regression_coefficient*X1_train)**2)/len(Y1_train)\n",
        "#print(\"Post Training Error: \", Post_training_error)\n",
        "\n",
        "test_error = np.sum((Y1_test - Regression_coefficient*X1_test)**2)/len(Y1_test)\n",
        "## Print the training and testing loss\n",
        "## update the below two variables to print the model's loss\n",
        "loss_train = Post_training_error\n",
        "loss_test = test_error\n",
        "\n",
        "print(\"Linear Regression\")\n",
        "print(\"Training Loss: \", loss_train)\n",
        "print(\"Testing  Loss: \", loss_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAp_AnW7cEhx",
        "outputId": "47c8edbd-02eb-4526-b409-99b0c0edb561"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta: [[2781.59003867]]\n",
            "Regression_coefficient:  [[66.1311054]]\n",
            "(70, 1)\n",
            "Linear Regression\n",
            "Training Loss:  27415.39655697824\n",
            "Testing  Loss:  44432.264978342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ones_column = np.ones((X1_train.shape[0], 1))\n",
        "X1_train_mod = np.hstack((ones_column, X1_train))\n",
        "\n",
        "ones_column = np.ones((X1_test.shape[0], 1))\n",
        "X1_test_mod = np.hstack((ones_column, X1_test))\n",
        "\n",
        "Y1_train_mod = np.resize(Y1_train, (len(Y1_train), 1))\n",
        "Y1_test_mod =  np.resize(Y1_test,  (len(Y1_test), 1))"
      ],
      "metadata": {
        "id": "-gyiUVn3Vwna"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This code snippet is to implement simple linear regression by implementing bias along with regression coefficient.\n",
        "#X1_train and X1_test has been modified to add 2nd column as ones, because x value is 1 for bias values.\n",
        "\n",
        "Regression_coefficients = np.dot(np.linalg.pinv(np.dot(X1_train_mod.T, X1_train_mod)), np.dot(X1_train_mod.T, Y1_train_mod))\n",
        "print(\"Regression_coefficient: \", Regression_coefficients)\n",
        "Y1_Lin_Bias = np.dot(X1_train_mod, Regression_coefficients)\n",
        "print(Y1_Lin_Bias.shape)\n",
        "\n",
        "## Compute the training and testing loss using MSE error\n",
        "Post_training_error = np.sum((Y1_train_mod - np.dot(X1_train_mod, Regression_coefficients))**2)/len(Y1_train_mod)\n",
        "print(\"Post Training Error: \", Post_training_error)\n",
        "test_error = np.sum((Y1_test_mod - np.dot(X1_test_mod, Regression_coefficients))**2)/len(Y1_test_mod)\n",
        "\n",
        "## Print the training and testing loss\n",
        "## update the below two variables to print the model's loss\n",
        "loss_train = Post_training_error\n",
        "loss_test = test_error\n",
        "print(\"Linear Regression\")\n",
        "print(\"Training Loss: \", loss_train)\n",
        "print(\"Testing  Loss: \", loss_test)\n"
      ],
      "metadata": {
        "id": "TrKwRWV8GbO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f599fca8-7711-43d5-c6ad-3332ce8bb06d"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression_coefficient:  [[47.21637965]\n",
            " [65.15344921]]\n",
            "(70, 1)\n",
            "Post Training Error:  25222.28423671403\n",
            "Linear Regression\n",
            "Training Loss:  25222.28423671403\n",
            "Testing  Loss:  41716.624929654376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Polynomial Regression (2 Marks)\n",
        "\n",
        "Solve the above problem using atleast 3 different hypothesis functions with polynomial degree 2, 3 and 4.\n",
        "\n",
        "Print out the train accuracy and test accuracy. Write your inference on the results.\n",
        "\n",
        "**Data**: X1_train, X1_test, Y1_train, Y1_test"
      ],
      "metadata": {
        "id": "yKbGL0zOGb2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ones_column = np.ones((X1_train.shape[0], 1))\n",
        "X1_train_poly2 = np.hstack((ones_column, X1_train, X1_train**2))\n",
        "print(X1_train_poly2[0:5])\n",
        "\n",
        "ones_column = np.ones((X1_test.shape[0], 1))\n",
        "X1_test_poly2 = np.hstack((ones_column, X1_test, X1_test**2))\n",
        "print(X1_test_poly2[0:3])\n",
        "\n",
        "Y1_train_poly2 = np.resize(Y1_train, (len(Y1_train), 1))\n",
        "Y1_test_poly2 =  np.resize(Y1_test,  (len(Y1_test), 1))\n",
        "print(X1_train_poly2.shape)\n",
        "print(X1_test_poly2.shape)\n",
        "print(Y1_train_poly2.shape)\n",
        "print(Y1_test_poly2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp_8UuCp0-Ls",
        "outputId": "c6eebf22-a6c8-44d4-e8f4-b6e32706d18d"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.         -8.85376401 78.38913723]\n",
            " [ 1.         -2.5895495   6.70576661]\n",
            " [ 1.          8.08186325 65.31651353]\n",
            " [ 1.         -4.97550386 24.75563861]\n",
            " [ 1.          5.49334111 30.17679656]]\n",
            "[[ 1.          6.26850875 39.29420194]\n",
            " [ 1.         -5.75615029 33.13326616]\n",
            " [ 1.         -5.98513959 35.82189591]]\n",
            "(70, 3)\n",
            "(30, 3)\n",
            "(70, 1)\n",
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Degree 2"
      ],
      "metadata": {
        "id": "XvczoOYN2nxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define the hypothesis function\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2]]\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "#Using Normal equation, Regression_cofficients = inv(X1_train.T*X1_train)*X1_train.T*Y1_train\n",
        "Regression_coefficients_poly2 = np.dot(np.linalg.pinv(np.dot(X1_train_poly2.T, X1_train_poly2)), np.dot(X1_train_poly2.T, Y1_train_poly2))\n",
        "print(\"Regression_coefficient: \", Regression_coefficients_poly2)\n",
        "Y1_Poly2 = np.dot(X1_train_poly2, Regression_coefficients_poly2)\n",
        "print(Y1_Poly2.shape)\n",
        "\n",
        "## Compute the training and testing loss\n",
        "Training_error = np.sum((Y1_train_poly2 - np.dot(X1_train_poly2, Regression_coefficients_poly2))**2)/len(Y1_train_poly2)\n",
        "test_error = np.sum((Y1_test_poly2 - np.dot(X1_test_poly2, Regression_coefficients_poly2))**2)/len(Y1_test_poly2)\n",
        "\n",
        "\n",
        "## Print the training and testing loss\n",
        "## update the below two variables to print the model's loss\n",
        "loss_train = Training_error\n",
        "loss_test = test_error\n",
        "\n",
        "print(\"Polynomial Degree 2\")\n",
        "print(\"Training loss: \", loss_train)\n",
        "print(\"Testing  loss: \", loss_test)"
      ],
      "metadata": {
        "id": "gqbgoOnQGeCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ec1b80-8384-49c5-f10f-789070dff2ed"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression_coefficient:  [[-5.81412195]\n",
            " [64.01720536]\n",
            " [ 1.42086134]]\n",
            "(70, 1)\n",
            "Polynomial Degree 2\n",
            "Training loss:  23308.50305148525\n",
            "Testing  loss:  36380.348114078115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ones_column = np.ones((X1_train.shape[0], 1))\n",
        "X1_train_poly3 = np.hstack((ones_column, X1_train, X1_train**2, X1_train**3))\n",
        "print(X1_train_poly3[0:5])\n",
        "\n",
        "ones_column = np.ones((X1_test.shape[0], 1))\n",
        "X1_test_poly3 = np.hstack((ones_column, X1_test, X1_test**2, X1_test**3))\n",
        "print(X1_test_poly3[0:3])\n",
        "\n",
        "Y1_train_poly3 = np.resize(Y1_train, (len(Y1_train), 1))\n",
        "Y1_test_poly3 =  np.resize(Y1_test,  (len(Y1_test), 1))\n",
        "print(X1_train_poly3.shape)\n",
        "print(X1_test_poly3.shape)\n",
        "print(Y1_train_poly3.shape)\n",
        "print(Y1_test_poly3.shape)"
      ],
      "metadata": {
        "id": "QxLyLbpnFM-L",
        "outputId": "45cff6ff-291d-4007-da3b-e2c9958788f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   1.           -8.85376401   78.38913723 -694.03892235]\n",
            " [   1.           -2.5895495     6.70576661  -17.36491455]\n",
            " [   1.            8.08186325   65.31651353  527.87913012]\n",
            " [   1.           -4.97550386   24.75563861 -123.17177536]\n",
            " [   1.            5.49334111   30.17679656  165.77143712]]\n",
            "[[   1.            6.26850875   39.29420194  246.31604869]\n",
            " [   1.           -5.75615029   33.13326616 -190.72005959]\n",
            " [   1.           -5.98513959   35.82189591 -214.39904741]]\n",
            "(70, 4)\n",
            "(30, 4)\n",
            "(70, 1)\n",
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Degree 3"
      ],
      "metadata": {
        "id": "_ct7OydN3ipg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define the hypothesis function\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2] + Beta_3 * X1_train[3]]\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2, Beta_3]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "#Using Normal equation, Regression_cofficients = inv(X1_train.T*X1_train)*X1_train.T*Y1_train\n",
        "Regression_coefficients_poly3 = np.dot(np.linalg.pinv(np.dot(X1_train_poly3.T, X1_train_poly3)), np.dot(X1_train_poly3.T, Y1_train_poly3))\n",
        "print(\"Regression_coefficient: \", Regression_coefficients_poly3)\n",
        "\n",
        "Y1_Poly3 = np.dot(X1_train_poly3, Regression_coefficients_poly3)\n",
        "print(Y1_Poly3.shape)\n",
        "\n",
        "## Compute the training and testing loss\n",
        "Training_error = np.sum((Y1_train_poly3 - np.dot(X1_train_poly3, Regression_coefficients_poly3))**2)/len(Y1_train_poly3)\n",
        "test_error = np.sum((Y1_test_poly3 - np.dot(X1_test_poly3, Regression_coefficients_poly3))**2)/len(Y1_test_poly3)\n",
        "\n",
        "\n",
        "## Print the training and testing loss\n",
        "## update the below two variables to print the model's loss\n",
        "loss_train = Training_error\n",
        "loss_test = test_error\n",
        "\n",
        "print(\"Polynomial Degree 3\")\n",
        "print(\"Training Loss: \", loss_train)\n",
        "print(\"Testing  Loss: \", loss_test)"
      ],
      "metadata": {
        "id": "X3pKkHvh2yS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2599d2b1-33a1-43e0-ebc5-6d1741bfcb41"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression_coefficient:  [[-4.29736247e-11]\n",
            " [ 1.00000000e+00]\n",
            " [ 1.00000000e+00]\n",
            " [ 1.00000000e+00]]\n",
            "(70, 1)\n",
            "Polynomial Degree 3\n",
            "Training Loss:  7.92854562673667e-22\n",
            "Testing  Loss:  8.283010751786646e-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Degree 4"
      ],
      "metadata": {
        "id": "KNPJfmG93nvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ones_column = np.ones((X1_train.shape[0], 1))\n",
        "X1_train_poly4 = np.hstack((ones_column, X1_train, X1_train**2, X1_train**3, X1_train**4))\n",
        "print(X1_train_poly4[0:5])\n",
        "\n",
        "ones_column = np.ones((X1_test.shape[0], 1))\n",
        "X1_test_poly4 = np.hstack((ones_column, X1_test, X1_test**2, X1_test**3, X1_test**4))\n",
        "print(X1_test_poly4[0:3])\n",
        "\n",
        "Y1_train_poly4 = np.resize(Y1_train, (len(Y1_train), 1))\n",
        "Y1_test_poly4 =  np.resize(Y1_test,  (len(Y1_test), 1))\n",
        "print(X1_train_poly4.shape)\n",
        "print(X1_test_poly4.shape)\n",
        "print(Y1_train_poly4.shape)\n",
        "print(Y1_test_poly4.shape)"
      ],
      "metadata": {
        "id": "qrEPclDFJLUQ",
        "outputId": "0d87da98-d93d-4091-bcd9-e615fafad55c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.00000000e+00 -8.85376401e+00  7.83891372e+01 -6.94038922e+02\n",
            "   6.14485684e+03]\n",
            " [ 1.00000000e+00 -2.58954950e+00  6.70576661e+00 -1.73649146e+01\n",
            "   4.49673058e+01]\n",
            " [ 1.00000000e+00  8.08186325e+00  6.53165135e+01  5.27879130e+02\n",
            "   4.26624694e+03]\n",
            " [ 1.00000000e+00 -4.97550386e+00  2.47556386e+01 -1.23171775e+02\n",
            "   6.12841643e+02]\n",
            " [ 1.00000000e+00  5.49334111e+00  3.01767966e+01  1.65771437e+02\n",
            "   9.10639051e+02]]\n",
            "[[ 1.00000000e+00  6.26850875e+00  3.92942019e+01  2.46316049e+02\n",
            "   1.54403431e+03]\n",
            " [ 1.00000000e+00 -5.75615029e+00  3.31332662e+01 -1.90720060e+02\n",
            "   1.09781333e+03]\n",
            " [ 1.00000000e+00 -5.98513959e+00  3.58218959e+01 -2.14399047e+02\n",
            "   1.28320823e+03]]\n",
            "(70, 5)\n",
            "(30, 5)\n",
            "(70, 1)\n",
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define the hypothesis function\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2] + Beta_3 * X1_train[3]]\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2, Beta_3]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "#Using Normal equation, Regression_cofficients = inv(X1_train.T*X1_train)*X1_train.T*Y1_train\n",
        "Regression_coefficients_poly4 = np.dot(np.linalg.pinv(np.dot(X1_train_poly4.T, X1_train_poly4)), np.dot(X1_train_poly4.T, Y1_train_poly4))\n",
        "print(\"Regression_coefficient: \", Regression_coefficients_poly4)\n",
        "Y1_Poly4 = np.dot(X1_train_poly4, Regression_coefficients_poly4)\n",
        "print(Y1_Poly4.shape)\n",
        "\n",
        "## Compute the training and testing loss\n",
        "Training_error = np.sum((Y1_train_poly4 - np.dot(X1_train_poly4, Regression_coefficients_poly4))**2)/len(Y1_train_poly4)\n",
        "test_error = np.sum((Y1_test_poly4 - np.dot(X1_test_poly4, Regression_coefficients_poly4))**2)/len(Y1_test_poly4)\n",
        "\n",
        "\n",
        "## Print the training and testing loss\n",
        "## update the below two variables to print the model's loss\n",
        "loss_train = Training_error\n",
        "loss_test = test_error\n",
        "\n",
        "print(\"Polynomial Degree 4\")\n",
        "print(\"Training Loss: \", loss_train)\n",
        "print(\"Testing  Loss: \", loss_test)"
      ],
      "metadata": {
        "id": "mMHzekphGhcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f06b0b-7b88-40a7-a4c5-eb612650d076"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression_coefficient:  [[1.04554861e-10]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [4.63908321e-14]]\n",
            "(70, 1)\n",
            "Polynomial Degree 4\n",
            "Training Loss:  2.786446139990528e-21\n",
            "Testing  Loss:  2.7485570890912254e-21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the coefficients (Theta) using the normal equation\n",
        "X_transpose_X = np.dot(X1_train_poly4.T, X1_train_poly4)\n",
        "X_transpose_y = np.dot(X1_train_poly4.T, Y1_train_poly4)\n",
        "coefficients = np.linalg.solve(X_transpose_X, X_transpose_y)\n",
        "\n",
        "# The coefficients represent the coefficients of the polynomial terms in increasing order of degree\n",
        "print(\"Coefficients for Polynomial Regression:\")\n",
        "for i, coeff in enumerate(coefficients):\n",
        "    print(f\"Theta_{i}:\", coeff)\n",
        "\n",
        "## Compute the training and testing loss\n",
        "Training_error = np.sum((Y1_train_poly4 - np.dot(X1_train_poly4, Regression_coefficients_poly4))**2)/len(Y1_train_poly4)\n",
        "test_error = np.sum((Y1_test_poly4 - np.dot(X1_test_poly4, Regression_coefficients_poly4))**2)/len(Y1_test_poly4)\n",
        "\n",
        "\n",
        "## Print the training and testing loss\n",
        "## update the below two variables to print the model's loss\n",
        "loss_train = Training_error\n",
        "loss_test = test_error\n",
        "\n",
        "print(\"Polynomial Degree 4\")\n",
        "print(\"Training Loss: \", loss_train)\n",
        "print(\"Testing  Loss: \", loss_test)"
      ],
      "metadata": {
        "id": "r6Qo6jVWO94W",
        "outputId": "53400d24-b2cf-45bf-9c05-d6ba9fbcc101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients for Polynomial Regression:\n",
            "Theta_0: [-1.14347785e-12]\n",
            "Theta_1: [1.]\n",
            "Theta_2: [1.]\n",
            "Theta_3: [1.]\n",
            "Theta_4: [-9.58115424e-16]\n",
            "Polynomial Degree 4\n",
            "Training Loss:  2.786446139990528e-21\n",
            "Testing  Loss:  2.7485570890912254e-21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "The training error and testing error changes with degree of polynomial. Between Simple linear regression, the hypothesis function without bias and with bias, the latter showed higher accuracy in terms of lower MSE than without bias. This confirms the observation that simple linear regression with bias is always better than the one without bias.\n",
        "Comparing Polynomial regression with degree 2,3 and 4, the training error and test error for degree 3 is the lowest, followed by degree 4 and then degree 2.\n",
        "Training and test error are both ~e-21. So the model isnt overfitting or underfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "W1Metdx-4esF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Overfitting and Underfitting (2 Marks)\n",
        "\n",
        "**Let us say we have a dataset with little noise. Then a model is underfitting when:**\n",
        "\n",
        "a) both the train and test errors are high\n",
        "\n",
        "b) train error is low but test error is high\n",
        "\n",
        "c) train error is high but the test error is low\n",
        "\n",
        "d) both train and test errors are low\n",
        "\n",
        "\n",
        "**If we choose the parameters of a model to get the best overfitting/underfitting tradeoff, we will always get a zero test error.**\n",
        "\n",
        "a) True\n",
        "\n",
        "b) False\n",
        "\n",
        "\n",
        "**State which of the below statements is false with respect to underfitting vs overfitting.**\n",
        "\n",
        "a) If the training set performance is roughly equal, underfitting is generally better.\n",
        "\n",
        "b) An underfit model is simpler and can usually be improved by looking at where it fits badly.\n",
        "\n",
        "c) An overfit model is often easier to change because it is easy to know where to start simplifying.\n",
        "\n",
        "d) Neither overfitting nor underfitting is desirable.\n"
      ],
      "metadata": {
        "id": "ZwBMt6VJGeW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write your answers below\n",
        "For the dataset with little noise, it underfits **a) when both the train and test errors are high**\n",
        "\n",
        "If we choose the parameters of a model to get the best overfitting/underfitting tradeoff, we will always get a zero test error. **FALSE**\n",
        "\n",
        "State which of the below statements is false with respect to underfitting vs overfitting:\n",
        "\n",
        "**d) Neither overfitting nor underfitting is desirable.**"
      ],
      "metadata": {
        "id": "gBwZRyYwMMYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4: Regularization (2 Marks)\n",
        "\n",
        "Solve the problem using regularization (Lasso, Ridge and Elastic net) on the 3 polynomial functions defined in **Problem 2**.\n",
        "\n",
        "Print out the train and test accuracy. Write your inference on the results\n",
        "\n",
        "**Data**: X1_train, X1_test, Y1_train, Y1_test\n"
      ],
      "metadata": {
        "id": "c1p1JzJWGhvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Degree 2"
      ],
      "metadata": {
        "id": "Sgh3esHf5Mmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lasso Regularization"
      ],
      "metadata": {
        "id": "7FYVCtRh6tvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2]]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.0001  # Learning rate\n",
        "lambda_ = 0.5  # Regularization parameter (lambda is L2 penalty)\n",
        "num_iterations = 50000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly2.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly2.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  3 coefficients [Beta_0, Beta_1, Beta_2]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly2, Beta)\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly2\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization for Lasso or L1 regularization\n",
        "    L1_penalty = (lambda_ / m) * np.sign(Beta)  # Derivative of |theta|\n",
        "#    print(L1_penalty.shape)\n",
        "    gradient = (1/m) * np.dot(X1_train_poly2.T, errors) + L1_penalty\n",
        "#    print(gradient.shape)\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Lasso Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly2 - np.dot(X1_train_poly2, Beta))**2)/len(Y1_train_poly2)\n",
        "Test_error = np.sum((Y1_test_poly2 - np.dot(X1_test_poly2, Beta))**2)/len(Y1_test_poly2)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "print(\"Polynomial Degree 2 : Lasso Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "hqiE5Hij5Mmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1f4fcf-1d5c-401e-d060-4acc61b34128"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Coefficients: [[-5.05374351]\n",
            " [64.02024458]\n",
            " [ 1.4088178 ]]\n",
            "Polynomial Degree 2 : Lasso Regularization\n",
            "Training Accuracy:  23308.73562449684\n",
            "Testing  Accuracy:  36406.89051235402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge Regularization"
      ],
      "metadata": {
        "id": "ZKHPfHa962C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2]]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.0001  # Learning rate\n",
        "lambda_ = 1 # Regularization parameter (lambda is L2 penalty)\n",
        "num_iterations = 50000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly2.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly2.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  3 coefficients [Beta_0, Beta_1, Beta_2]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly2, Beta)\n",
        "\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly2\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization (except for theta_0)\n",
        "    gradient = (1/m) * np.dot(X1_train_poly2.T, errors) + (lambda_/m) * Beta\n",
        "#   print(gradient.shape)\n",
        "    gradient[0] = (1/m) *np.sum(errors)  # Exclude regularization for theta_0\n",
        "    #print(gradient)\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Ridge Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly2 - np.dot(X1_train_poly2, Beta))**2)/len(Y1_train_poly2)\n",
        "Test_error = np.sum((Y1_test_poly2 - np.dot(X1_test_poly2, Beta))**2)/len(Y1_test_poly2)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "print(\"Polynomial Degree 2 : Ridge Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnRQYDDaYTVv",
        "outputId": "abaff3fb-b828-4f54-b364-ba602e20a17c"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Coefficients: [[-5.07684928]\n",
            " [63.99528795]\n",
            " [ 1.40979339]]\n",
            "Polynomial Degree 2 : Ridge Regularization\n",
            "Training Accuracy:  23308.744523883535\n",
            "Testing  Accuracy:  36417.16680475828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elastic net Regularization"
      ],
      "metadata": {
        "id": "_uE1UbuG61cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2]]\n",
        "#Hypothesis = [Beta] * X1_train_poly2\n",
        "#X1_test_poly4 = np.hstack((ones_column, X1_test, X1_test**2))\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.0001  # Learning rate\n",
        "lambda_l1 = 0.1  # L1 regularization parameter is lasso parameter (for feature selection)\n",
        "lambda_l2 = 0.1  # L2 regularization parameter is ridge parameter (for handling correlated features)\n",
        "num_iterations = 1000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly2.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly2.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  3 coefficients [Beta_0, Beta_1, Beta_2]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly2, Beta)\n",
        "\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly2\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization (except for Beta_0) with Elastic Net regularization\n",
        "    L1_penalty = (lambda_l1 / m) * np.sign(Beta[1:])  # Exclude Bias term from L1 regularization\n",
        "    L2_penalty = (lambda_l2 / m) * Beta[1:]  # Exclude Bias term from L2 regularization\n",
        "    gradient = (1/m) * np.dot(X1_train_poly2.T, errors)\n",
        "    gradient[1:] += L1_penalty + L2_penalty  # Apply both L1 and L2 regularization\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Elastic Net Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly2 - np.dot(X1_train_poly2, Beta))**2)/len(Y1_train_poly2)\n",
        "Test_error = np.sum((Y1_test_poly2 - np.dot(X1_test_poly2, Beta))**2)/len(Y1_test_poly2)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "print(\"Polynomial Degree 2 : Elastic net Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "0Hu8O10s61cG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215a4e01-8816-4e98-e87a-d6a6971dd458"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Coefficients: [[-0.46260615]\n",
            " [62.37832355]\n",
            " [ 1.37782283]]\n",
            "Polynomial Degree 2 : Elastic net Regularization\n",
            "Training Accuracy:  23420.69703181681\n",
            "Testing  Accuracy:  37424.36705255984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Degree 3"
      ],
      "metadata": {
        "id": "eyiLYqgo7Kcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lasso Regularization"
      ],
      "metadata": {
        "id": "71-KPDCE7Kc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2]]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.00001  # Learning rate\n",
        "lambda_ = 5  # Regularization parameter (lambda is L2 penalty)\n",
        "num_iterations = 50000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly3.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly3.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  3 coefficients [Beta_0, Beta_1, Beta_2]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly3, Beta)\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly3\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization for Lasso or L1 regularization\n",
        "    L1_penalty = (lambda_ / m) * np.sign(Beta)  # Derivative of |theta|\n",
        "#    print(L1_penalty.shape)\n",
        "    gradient = (1/m) * np.dot(X1_train_poly3.T, errors) + L1_penalty\n",
        "#    print(gradient.shape)\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Lasso Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly3 - np.dot(X1_train_poly3, Beta))**2)/len(Y1_train_poly3)\n",
        "Test_error = np.sum((Y1_test_poly3 - np.dot(X1_test_poly3, Beta))**2)/len(Y1_test_poly3)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "\n",
        "print(\"Polynomial Degree 3 : Lasso Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "Z2VyClSM7Kc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015a6eed-5d2b-4f84-e3fe-c97c1ec56ad1"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Coefficients: [[-4.01246630e-07]\n",
            " [ 9.08290955e-01]\n",
            " [ 9.99871882e-01]\n",
            " [ 1.00125354e+00]]\n",
            "Polynomial Degree 3 : Lasso Regularization\n",
            "Training Accuracy:  0.04251768050555191\n",
            "Testing  Accuracy:  0.05687734447897456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge Regularization"
      ],
      "metadata": {
        "id": "Lej8AznZ7Kc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2]]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.0001  # Learning rate\n",
        "lambda_ = 1 # Regularization parameter (lambda is L2 penalty)\n",
        "num_iterations = 50000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly3.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly2.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  3 coefficients [Beta_0, Beta_1, Beta_2]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly3, Beta)\n",
        "\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly3\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization (except for theta_0)\n",
        "    gradient = (1/m) * np.dot(X1_train_poly3.T, errors) + (lambda_/m) * Beta\n",
        "#   print(gradient.shape)\n",
        "    gradient[0] = (1/m) *np.sum(errors)  # Exclude regularization for theta_0\n",
        "    #print(gradient)\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Ridge Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly3 - np.dot(X1_train_poly3, Beta))**2)/len(Y1_train_poly3)\n",
        "Test_error = np.sum((Y1_test_poly3 - np.dot(X1_test_poly3, Beta))**2)/len(Y1_test_poly3)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "print(\"Polynomial Degree 3 : Ridge Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "plhN1IfeORFI",
        "outputId": "a3880f58-332b-42e1-98b7-94ece508cb98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-219-f692e378ed6e>:38: RuntimeWarning: invalid value encountered in subtract\n",
            "  Beta -= alpha*gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Coefficients: [[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Polynomial Degree 3 : Ridge Regularization\n",
            "Training Accuracy:  nan\n",
            "Testing  Accuracy:  nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elastic net Regularization"
      ],
      "metadata": {
        "id": "MFw7cIot7Kc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2, Beta_3, Beta_4]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2] + Beta_3 * X1_train[3] + Beta_4 * X1_train[4]]\n",
        "#Hypothesis = [Beta] * X1_train_poly4\n",
        "#X1_test_poly4 = np.hstack((ones_column, X1_test, X1_test**2, X1_test**3, X1_test**4))\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.0001  # Learning rate\n",
        "lambda_l1 = 0.1  # L1 regularization parameter is lasso parameter (for feature selection)\n",
        "lambda_l2 = 0.1  # L2 regularization parameter is ridge parameter (for handling correlated features)\n",
        "num_iterations = 1000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly3.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly2.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  4 coefficients [Beta_0, Beta_1, Beta_2, Beta_3]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly3, Beta)\n",
        "\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly3\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization (except for Beta_0) with Elastic Net regularization\n",
        "    L1_penalty = (lambda_l1 / m) * np.sign(Beta[1:])  # Exclude Bias term from L1 regularization\n",
        "    L2_penalty = (lambda_l2 / m) * Beta[1:]  # Exclude Bias term from L2 regularization\n",
        "    gradient = (1/m) * np.dot(X1_train_poly3.T, errors)\n",
        "    gradient[1:] += L1_penalty + L2_penalty  # Apply both L1 and L2 regularization\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Elastic Net Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly3 - np.dot(X1_train_poly3, Beta))**2)/len(Y1_train_poly3)\n",
        "Test_error = np.sum((Y1_test_poly3 - np.dot(X1_test_poly3, Beta))**2)/len(Y1_test_poly3)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "print(\"Polynomial Degree 3 : Elastic net Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "Zo-WrnJv7Kc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a6e344-cd92-415c-e913-17bbfd31fdc6"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Coefficients: [[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Polynomial Degree 3 : Elastic net Regularization\n",
            "Training Accuracy:  nan\n",
            "Testing  Accuracy:  nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-220-a8c250ff51e0>:39: RuntimeWarning: invalid value encountered in subtract\n",
            "  Beta -= alpha*gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Degree 4"
      ],
      "metadata": {
        "id": "y7WAezGP7LX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lasso Regularization"
      ],
      "metadata": {
        "id": "5HJW1wl-7LX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2]]\n",
        "#Hypothesis = [Beta] * X1_train\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.00000001  # Learning rate\n",
        "lambda_ = 1  # Regularization parameter (lambda is L2 penalty)\n",
        "num_iterations = 50000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly4.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly3.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  3 coefficients [Beta_0, Beta_1, Beta_2]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly4, Beta)\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly4\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization for Lasso or L1 regularization\n",
        "    L1_penalty = (lambda_ / m) * np.sign(Beta)  # Derivative of |theta|\n",
        "#    print(L1_penalty.shape)\n",
        "    gradient = (1/m) * np.dot(X1_train_poly4.T, errors) + L1_penalty\n",
        "#    print(gradient.shape)\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Lasso Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly4 - np.dot(X1_train_poly4, Beta))**2)/len(Y1_train_poly4)\n",
        "Test_error = np.sum((Y1_test_poly4 - np.dot(X1_test_poly4, Beta))**2)/len(Y1_test_poly4)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "\n",
        "print(\"Polynomial Degree 4 : Lasso Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "YkIJpStm7LYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942a8f63-d2c9-4b23-f4fb-e086598eaaa7"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Coefficients: [[0.00346592]\n",
            " [0.01753696]\n",
            " [0.0774439 ]\n",
            " [1.01313201]\n",
            " [0.01172984]]\n",
            "Polynomial Degree 4 : Lasso Regularization\n",
            "Training Accuracy:  144.73021732768845\n",
            "Testing  Accuracy:  222.93692085429433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge Regularization"
      ],
      "metadata": {
        "id": "1dReUsBK7LYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2] + Beta_3 * X1_train[3] + Beta_4 * X1_train[4]]\n",
        "#Hypothesis = [Beta] * X1_train_poly4\n",
        "#X1_test_poly4 = np.hstack((ones_column, X1_test, X1_test**2, X1_test**3, X1_test**4))\n",
        "\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.0000001  # Learning rate\n",
        "lambda_ = 10 # Regularization parameter (lambda is L2 penalty)\n",
        "num_iterations = 50000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly4.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly2.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  3 coefficients [Beta_0, Beta_1, Beta_2]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly4, Beta)\n",
        "\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly4\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization (except for theta_0)\n",
        "    gradient = (1/m) * np.dot(X1_train_poly4.T, errors) + (lambda_/m) * Beta\n",
        "#   print(gradient.shape)\n",
        "    gradient[0] = (1/m) *np.sum(errors)  # Exclude regularization for theta_0\n",
        "    #print(gradient)\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Ridge Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly4 - np.dot(X1_train_poly4, Beta))**2)/len(Y1_train_poly4)\n",
        "Test_error = np.sum((Y1_test_poly4 - np.dot(X1_test_poly4, Beta))**2)/len(Y1_test_poly4)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "print(\"Polynomial Degree 4 : Ridge Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "CgKlKFvM7LYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c512cf-e232-409a-a6a8-f209aa7493b8"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Coefficients: [[0.02489389]\n",
            " [0.04675362]\n",
            " [0.55622444]\n",
            " [1.01290007]\n",
            " [0.00562428]]\n",
            "Polynomial Degree 4 : Ridge Regularization\n",
            "Training Accuracy:  37.84812673901232\n",
            "Testing  Accuracy:  56.78742947883662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elastic net Regularization"
      ],
      "metadata": {
        "id": "HgkNRup17LYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "## Define a new hypothesis function by adding a Lasso regularizer\n",
        "## to the 2nd order hypothesis function that you defined earlier\n",
        "\n",
        "#Regression_cofficients = Beta = [Beta_0, Beta_1, Beta_2, Beta_3, Beta_4]\n",
        "#Hypothesis = [Beta_0 * X1_train[0] + Beta_1 * X1_train[1] + Beta_2 * X1_train[2] + Beta_3 * X1_train[3] + Beta_4 * X1_train[4]]\n",
        "#Hypothesis = [Beta] * X1_train_poly4\n",
        "#X1_test_poly4 = np.hstack((ones_column, X1_test, X1_test**2, X1_test**3, X1_test**4))\n",
        "## Use polynomial regression to train the model\n",
        "\n",
        "# Define (hyper)parameters for Gradient descent algorithm\n",
        "alpha = 0.000001  # Learning rate\n",
        "lambda_l1 = 1  # L1 regularization parameter is lasso parameter (for feature selection)\n",
        "lambda_l2 = 10  # L2 regularization parameter is ridge parameter (for handling correlated features)\n",
        "num_iterations = 50000 #Nr of iteration for GD to run\n",
        "m, n = X1_train_poly4.shape # no_of_training_examples, no_of_features\n",
        "\n",
        "#print(X1_test_poly2.T.shape)\n",
        "# Initialize coefficients\n",
        "Beta = np.zeros(n)  #  5 coefficients [Beta_0, Beta_1, Beta_2,Beta_3, Beta_4]\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "print(Beta.shape)\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "# Calculate predictions\n",
        "    predictions = np.dot(X1_train_poly4, Beta)\n",
        "\n",
        "\n",
        "# Compute errors\n",
        "    errors = predictions - Y1_train_poly4\n",
        "    #print(errors.shape)\n",
        "# Compute gradient with regularization (except for Beta_0) with Elastic Net regularization\n",
        "    L1_penalty = (lambda_l1 / m) * np.sign(Beta[1:])  # Exclude Bias term from L1 regularization\n",
        "    L2_penalty = (lambda_l2 / m) * Beta[1:]  # Exclude Bias term from L2 regularization\n",
        "    gradient = (1/m) * np.dot(X1_train_poly4.T, errors)\n",
        "    gradient[1:] += L1_penalty + L2_penalty  # Apply both L1 and L2 regularization\n",
        "# Update coefficients meaning bias and weights\n",
        "    Beta -= alpha*gradient\n",
        "# Print coefficients\n",
        "print(\"Elastic Net Coefficients:\", Beta)\n",
        "\n",
        "## Compute the training and testing accuracy\n",
        "Training_error = np.sum((Y1_train_poly4 - np.dot(X1_train_poly4, Beta))**2)/len(Y1_train_poly4)\n",
        "Test_error = np.sum((Y1_test_poly4 - np.dot(X1_test_poly4, Beta))**2)/len(Y1_test_poly4)\n",
        "\n",
        "## Print the training and testing accuracy\n",
        "## update the below two variables to print the model's accuracy\n",
        "accuracy_train = Training_error\n",
        "accuracy_test = Test_error\n",
        "\n",
        "print(\"Polynomial Degree 4 : Elastic net Regularization\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Testing  Accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "CEYHNkcU7LYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4db8b5-891e-4bde-bdab-5d4f9ffc0385"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-230-d839b4eec0b9>:39: RuntimeWarning: invalid value encountered in subtract\n",
            "  Beta -= alpha*gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Coefficients: [[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Polynomial Degree 4 : Elastic net Regularization\n",
            "Training Accuracy:  nan\n",
            "Testing  Accuracy:  nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m-yrbklZj3Tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "*Write your inference here*\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "4N5dzLiF8VEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 5: Logistic Regression (2 Marks)\n",
        "\n",
        "Train a logistic regression model.\n",
        "Print the F1 score, accuracy and confusion matrix for both training and testing.\n",
        "\n",
        "**Data**: X2_train, X2_test, Y2_train, Y2_test"
      ],
      "metadata": {
        "id": "-nGbUYoTs29F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X2_train.shape)\n",
        "print(X2_test.shape)\n",
        "print(Y2_train.shape)\n",
        "print(Y2_test.shape)"
      ],
      "metadata": {
        "id": "VcdxaFB_5R7M",
        "outputId": "27627227-f3b9-4994-d76c-9a49a91e675e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 20)\n",
            "(30, 20)\n",
            "(70, 1)\n",
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Write your code here\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_iterations = 1000\n",
        "\n",
        "X2_train_Bias = np.column_stack([np.ones(X2_train.shape[0]), X2_train])\n",
        "#print(\"X2_train_Bias: \",X2_train_Bias.shape)\n",
        "#print(X2_train_Bias[0])\n",
        "\n",
        "X2_test_Bias = np.column_stack([np.ones(X2_test.shape[0]), X2_test])\n",
        "#print(\"X2_test_Bias:\",X2_test_Bias.shape)\n",
        "\n",
        "# Initialize coefficients (including the intercept)\n",
        "m, n = X2_train_Bias.shape # no_of_training_examples, no_of_features\n",
        "Beta = np.zeros(n)   # Regression Coefficients\n",
        "Beta = np.resize(Beta, (len(Beta), 1))\n",
        "#print(\"Beta:\",Beta.shape)\n",
        "# Add a column of ones for the Bias/intercept term in X2_train and X2_test\n",
        "\n",
        "# Sigmoid function (logistic function)\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Gradient Descent\n",
        "for _ in range(num_iterations):\n",
        "    # Calculate the linear combination of features and coefficients\n",
        "    linear_combination = np.dot(X2_train_Bias, Beta)\n",
        "# Apply the sigmoid function to get probabilities\n",
        "    probabilities = sigmoid(linear_combination)\n",
        "\n",
        "# Compute errors (log loss)\n",
        "    errors = -(Y2_train - probabilities)\n",
        "\n",
        "# Compute gradients\n",
        "    gradient = (1/m) * np.dot(X2_train_Bias.T, errors)\n",
        "#    print(gradient.shape)\n",
        "# Update coefficients (including the intercept)\n",
        "    Beta -= learning_rate * gradient\n",
        "\n",
        "# Extract the intercept and feature coefficients\n",
        "intercept = Beta[0]\n",
        "Logistic_regression_coefficients = Beta[1:]\n",
        "#print(Beta.shape)\n",
        "\n",
        "# Print the logistic regression coefficients\n",
        "#print(\"Logistic Regression Coefficients:\")\n",
        "#print(\"Intercept:\", intercept)\n",
        "#print(\"Coefficients:\", Logistic_regression_coefficients)\n",
        "\n",
        "## Compute f1 score, accuracy and confusion matrix for training\n",
        "\n",
        "Y2_train_pred = np.dot(X2_train_Bias, Beta)\n",
        "#print(Y2_train_pred.shape)\n",
        "#print(\"Y2_train_pred:\",Y2_train_pred[0:20])\n",
        "\n",
        "# Set a threshold (usually 0.5 for binary classification)\n",
        "threshold = 0.5\n",
        "\n",
        "# Convert probabilities to binary class labels\n",
        "predicted_labels = (Y2_train_pred >= threshold).astype(int)\n",
        "#print(predicted_labels[0:20])\n",
        "\n",
        "# Compute true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)\n",
        "# For Training data\n",
        "TP = np.sum((predicted_labels == 1) & (Y2_train == 1))\n",
        "TN = np.sum((predicted_labels == 0) & (Y2_train == 0))\n",
        "FP = np.sum((predicted_labels == 0) & (Y2_train == 1))\n",
        "FN = np.sum((predicted_labels == 1) & (Y2_train == 0))\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Compute confusion matrix\n",
        "confusion_matrix = np.array([[TN, FP], [FN, TP]])\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "## Print the above computed metrics\n",
        "accuracy_train = accuracy\n",
        "f1_train = f1\n",
        "cf_train = confusion_matrix # confusion matrix\n",
        "\n",
        "\n",
        "print(\"Logistic Regression\")\n",
        "print(\"Training Accuracy: \", accuracy_train)\n",
        "print(\"Training F1 score: \", f1_train)\n",
        "print(\"Training confusion matrix: \")\n",
        "print(cf_train)"
      ],
      "metadata": {
        "id": "vDFXbA2B_b8l",
        "outputId": "cedb8eec-8d1e-4f68-c8f9-25b9578a8a86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "Training Accuracy:  0.7285714285714285\n",
            "Training F1 score:  0.6545454545454547\n",
            "Training confusion matrix: \n",
            "[[33 19]\n",
            " [ 0 18]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Compute f1 score, accuracy and confusion matrix for testing\n",
        "\n",
        "Y2_test_pred = np.dot(X2_test_Bias, Beta)\n",
        "#print(Y2_train_pred.shape)\n",
        "#print(\"Y2_train_pred:\",Y2_train_pred[0:20])\n",
        "\n",
        "# Set a threshold (usually 0.5 for binary classification)\n",
        "threshold = 0.5\n",
        "\n",
        "# Convert probabilities to binary class labels\n",
        "predicted_labels = (Y2_test_pred >= threshold).astype(int)\n",
        "#print(predicted_labels[0:20])\n",
        "\n",
        "# Compute true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)\n",
        "# For Training data\n",
        "TP = np.sum((predicted_labels == 1) & (Y2_test == 1))\n",
        "TN = np.sum((predicted_labels == 0) & (Y2_test == 0))\n",
        "FP = np.sum((predicted_labels == 0) & (Y2_test == 1))\n",
        "FN = np.sum((predicted_labels == 1) & (Y2_test == 0))\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Compute confusion matrix\n",
        "confusion_matrix = np.array([[TN, FP], [FN, TP]])\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "## Print the above computed metrics\n",
        "accuracy_test = accuracy\n",
        "f1_test = f1\n",
        "cf_test = confusion_matrix # confusion matrix\n",
        "\n",
        "print(\"Logistic Regression\")\n",
        "print(\"Testing  Accuracy: \", accuracy_test)\n",
        "print(\"Testing  F1 score: \", f1_test)\n",
        "print(\"Testing  confusion matrix: \")\n",
        "print(cf_test)"
      ],
      "metadata": {
        "id": "69xRrEt3Ly1O",
        "outputId": "fc7e6668-4318-49ad-cb71-1bfc82f3b078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "Testing  Accuracy:  0.7666666666666667\n",
            "Testing  F1 score:  0.7200000000000001\n",
            "Testing  confusion matrix: \n",
            "[[14  6]\n",
            " [ 1  9]]\n"
          ]
        }
      ]
    }
  ]
}