{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntp3105/IISc-CCE-ML-AI-MLOps/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Kafka and SPARK"
      ],
      "metadata": {
        "id": "0gTv-7nwLC8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1 (7 marks)\n",
        "\n",
        "1. Setup Apache Kafka and Apache Spark in your local system.\n",
        "2. Install and start Kafka and Zookeper.\n",
        "3. Create a kafka topic named \"assignment-4\".\n",
        "4. Start the Kafka producer and write few events into the topic.\n",
        "5. Consume the events using Spark structured streaming.\n",
        "6. Document all the steps and the commands used in markdown format including all your codes.\n",
        "\n",
        "## Use the below format\n",
        "\n",
        "- Steps/command description\n",
        "```\n",
        "<command>\n",
        "```\n",
        "\n",
        "- *File: <FileName.py>*\n",
        "``` file\n",
        "codes...\n",
        "```"
      ],
      "metadata": {
        "id": "1u3IftTwekW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2 (3 Marks)\n",
        "\n",
        "\n",
        "What is the expected output of the following PySpark code? What would be the values stored inside `marksRDD`, `courseRDD` and `pairRDD`? *(theoretical question, you are free to implement it though)*\n",
        "\n",
        "```\n",
        "marksRDD = sc.textFile(“marks.csv”);\n",
        "courseRDD = marksRDD.filter(lambda x : x.split(“,”)[1] == “DA204”);\n",
        "pairRDD = courseRDD.map(lambda y :  \n",
        "( y.split(“,”)[0], float(y.split(“,”)[2]) ) );\n",
        "gradesRDD = pairRDD.map(lambda k,v:  \n",
        "  (k, “A”) if v > 80 else  \n",
        "    ((k, “B”) if v > 70 else  \n",
        "       ((k, “C”) if v > 60 else (k, “D”))\n",
        "    )\n",
        "  );\n",
        "\n",
        "gradesRDD.collect();\n",
        "```\n",
        "\n",
        "**Marks.csv**\n",
        "```\n",
        "1,DA204,90\n",
        "2,DS211,85\n",
        "3,DA204,50\n",
        "4,DS211,40\n",
        "5,DA204,70\n",
        "6,DS220,72\n",
        "7,DA204,65\n",
        "8,DS220,69\n",
        "9,DA204,75\n",
        "```"
      ],
      "metadata": {
        "id": "YXgI8bNNJcf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some useful references\n",
        "### Kafka\n",
        "- https://kafka.apache.org/quickstart\n",
        "- https://kafka.apache.org/documentation/#intro_concepts_and_terms\n",
        "\n",
        "### Spark\n",
        "- https://spark.apache.org/\n",
        "- https://medium.com/linkit-intecs/pyspark-with-google-colab-d964fd693ca7\n",
        "\n",
        "### Kafka with spark\n",
        "- https://adinasarapu.github.io/posts/2020/01/blog-post-kafka/\n",
        "- https://adinasarapu.github.io/posts/2021/02/blog-post-kafka-spark-streaming/\n",
        "\n",
        "### Kafka with tensorflow\n",
        "- https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/kafka.ipynb#scrollTo=f6qxCdypE1DD\n",
        "\n",
        "### Example(using docker):\n",
        "- https://github.com/cordon-thiago/spark-kafka-consumer\n"
      ],
      "metadata": {
        "id": "8zVsI5fHliv_"
      }
    }
  ]
}